// src/shared/api/ai/frontendAiProcessor.ts

import { AppDispatch } from "@/shared/config/store";
import {
  GestureResult,
  EmotionResult,
  AiSystemConfig,
  GestureCallback,
  EmotionCallback,
} from "@/shared/types/ai.types";

import { emotionCaptureManager } from "./emotionCaptureManager";
import { EmotionFaceProcessor } from "./emotionFaceProcessor";
import { BeautyFilterProcessor } from "./beautyFilterProcessor";
import { GestureProcessor } from "./gestureProcessor";
import * as tf from "@tensorflow/tfjs";

// --- AI ëª¨ë¸ ê²½ë¡œ ìƒìˆ˜ ì •ì˜ ---
// Next.jsì˜ public í´ë”ëŠ” ì„œë²„ ë£¨íŠ¸(/)ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.
const MODELS_BASE_PATH = "/models";

// í‘œì • ì¸ì‹ ëª¨ë¸ ê²½ë¡œ
const EXPRESSION_MODEL_PATH = `${MODELS_BASE_PATH}/expression/model.json`;
const EXPRESSION_SCALER_PATH = `${MODELS_BASE_PATH}/expression/scaler_v3.json`;

// ì œìŠ¤ì²˜ ì¸ì‹ ëª¨ë¸ ê²½ë¡œ (GestureProcessor ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë  ê²½ë¡œ)
const STATIC_GESTURE_MODEL_PATH = `${MODELS_BASE_PATH}/static-gesture/model.json`;
const DYNAMIC_GESTURE_MODEL_PATH = `${MODELS_BASE_PATH}/dinamic-gesture/model.json`; // ì‹¤ì œ í´ë”ëª…ì— ë§ì¶° ìˆ˜ì •

// MediaPipe WASM íŒŒì¼ CDN ê²½ë¡œ
const FACE_MESH_WASM_PATH = "https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh";
const TASKS_VISION_WASM_PATH = "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm";

class FrontendAiProcessor {
  private dispatch: AppDispatch | null = null;
  private aiConfig: AiSystemConfig = {
    gesture: {
      static: { enabled: true, confidence: 0.75 },
      dynamic: { enabled: true, confidence: 0.9 },
    },
    emotion: { enabled: false, confidence: 0.6 },
    beauty: { enabled: false, gamma: 1.4, lipAlpha: 0.2, smoothAmount: 30, lipColor: [255, 0, 0] },
  };

  private emotionFaceProcessor: EmotionFaceProcessor | null = null;
  private beautyFilterProcessor: BeautyFilterProcessor | null = null;
  private gestureProcessor: GestureProcessor | null = null;
  private isInitialized = false;

  private onGestureResultCallback: GestureCallback | null = null;
  private onEmotionResultCallback: EmotionCallback | null = null;

  private lastFrameTime = 0;
  private frameInterval = 100;
  
  // ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ìš© ë³€ìˆ˜ë“¤
  private backgroundVideoElement: HTMLVideoElement | null = null;
  private backgroundAnalysisActive = false;
  private backgroundAnalysisLoop: number | null = null;
  
  // AI ê²°ê³¼ í‘œì‹œ ì†ë„ ì œì–´
  private lastGestureResultTime = 0;
  private lastEmotionResultTime = 0;
  private readonly GESTURE_RESULT_INTERVAL = 1500; // ì œìŠ¤ì²˜ ê²°ê³¼ ê°„ê²© (ms)
  private readonly EMOTION_RESULT_INTERVAL = 1500; // ê°ì • ê²°ê³¼ ê°„ê²© (ms)

  private activeGestureEmojis: Map<string, any> = new Map();

  private readonly STATIC_GESTURE_DURATION = 1500;
  private readonly DYNAMIC_GESTURE_DURATION = 1500;
  private readonly ANIMATION_FADE_DURATION = 150;

  private activeSourceTrack: MediaStreamTrack | null = null; // í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì›ë³¸ íŠ¸ë™ ì €ì¥
  private activeProcessedTrack: MediaStreamTrack | null = null; // ìƒì„±ëœ AI ì²˜ë¦¬ íŠ¸ë™ ì €ì¥


  public async init(dispatch: AppDispatch): Promise<void> {
    this.dispatch = dispatch;
    emotionCaptureManager.init(dispatch);
    try {
      await tf.setBackend('webgl'); // WebGL ë°±ì—”ë“œ ì„¤ì •
      console.log(`TensorFlow.js backend set to: ${tf.getBackend()}`);
    } catch (error) {
      console.warn("âŒ Failed to set TensorFlow.js WebGL backend, falling back to CPU:", error);
      // WebGL ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ CPUë‚˜ WASMìœ¼ë¡œ í´ë°±ë  ìˆ˜ ìˆìœ¼ë‚˜, ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
      // await tf.setBackend('cpu'); // ë˜ëŠ” 'wasm'
    }

    this.emotionFaceProcessor = new EmotionFaceProcessor(this.aiConfig);
    this.beautyFilterProcessor = new BeautyFilterProcessor(this.aiConfig);
    this.gestureProcessor = new GestureProcessor(this.aiConfig);

    try {
      console.log("ğŸ¤– Initializing AI models...");
      console.log(`- Emotion model: ${EXPRESSION_MODEL_PATH}`);
      console.log(`- Emotion scaler: ${EXPRESSION_SCALER_PATH}`);

      await Promise.all([
        // ì •ì˜ëœ ê²½ë¡œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”
        this.emotionFaceProcessor.init(
          EXPRESSION_MODEL_PATH,
          EXPRESSION_SCALER_PATH,
          FACE_MESH_WASM_PATH
        ),
        this.beautyFilterProcessor.init(FACE_MESH_WASM_PATH),
        this.gestureProcessor.init(TASKS_VISION_WASM_PATH),
      ]);

      console.log(`- Static gesture model: ${STATIC_GESTURE_MODEL_PATH}`);
      console.log(`- Dynamic gesture model: ${DYNAMIC_GESTURE_MODEL_PATH}`);

      this.isInitialized = true;
      console.log("âœ… FrontendAiProcessor initialized successfully.");
    } catch (error) {
      console.error("âŒ FrontendAiProcessor: Failed to initialize AI models:", error);
      this.isInitialized = false;
    }
  }

  public updateConfig(config: Partial<AiSystemConfig>): Promise<void> {
    return Promise.resolve().then(() => {
      this.aiConfig = {
        ...this.aiConfig,
        ...config,
        gesture: { ...this.aiConfig.gesture, ...config.gesture },
        emotion: { ...this.aiConfig.emotion, ...config.emotion },
        beauty: { ...this.aiConfig.beauty, ...config.beauty },
      };

      this.emotionFaceProcessor?.updateConfig(this.aiConfig);
      this.beautyFilterProcessor?.updateConfig(this.aiConfig);
      this.gestureProcessor?.updateConfig(this.aiConfig);
    });
  }

  public setGestureCallback(callback: GestureCallback): void {
    this.onGestureResultCallback = callback;
  }

  public setEmotionCallback(callback: EmotionCallback): void {
    this.onEmotionResultCallback = callback;
  }

  /**
   * ë°±ê·¸ë¼ìš´ë“œì—ì„œ AI ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤ (íŠ¸ë™ êµì²´ ì—†ìŒ)
   * @param originalTrack ì›ë³¸ ë¹„ë””ì˜¤ íŠ¸ë™
   */
  public startBackgroundAnalysis(originalTrack: MediaStreamTrack): void {
    if (originalTrack.kind !== "video") {
      console.warn("Only video tracks can be analyzed.");
      return;
    }

    console.log("ğŸ¤– Starting background AI analysis...");
    
    // ê¸°ì¡´ ë¶„ì„ì´ ìˆë‹¤ë©´ ì¤‘ì§€
    this.stopBackgroundAnalysis();

    // ë°±ê·¸ë¼ìš´ë“œ ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ìƒì„±
    this.backgroundVideoElement = document.createElement("video");
    this.backgroundVideoElement.srcObject = new MediaStream([originalTrack]);
    this.backgroundVideoElement.autoplay = true;
    this.backgroundVideoElement.muted = true;
    this.backgroundVideoElement.style.display = "none"; // í™”ë©´ì— í‘œì‹œí•˜ì§€ ì•ŠìŒ

    this.backgroundVideoElement.onloadedmetadata = () => {
      this.backgroundAnalysisActive = true;
      this.runBackgroundAnalysisLoop();
      console.log("âœ… Background AI analysis started");
      
    };

    // ë¹„ë””ì˜¤ ì¬ìƒ ì‹œì‘
    this.backgroundVideoElement.play();
  }

  /**
   * ë°±ê·¸ë¼ìš´ë“œ AI ë¶„ì„ì„ ì¤‘ì§€í•©ë‹ˆë‹¤
   */
  public stopBackgroundAnalysis(): void {
    console.log("ğŸ›‘ Stopping background AI analysis...");
    
    this.backgroundAnalysisActive = false;
    
    if (this.backgroundAnalysisLoop) {
      cancelAnimationFrame(this.backgroundAnalysisLoop);
      this.backgroundAnalysisLoop = null;
    }

    if (this.backgroundVideoElement) {
      this.backgroundVideoElement.pause();
      this.backgroundVideoElement.srcObject = null;
      this.backgroundVideoElement = null;
    }

    console.log("âœ… Background AI analysis stopped");
  }

  /**
   * ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ë£¨í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤
   */
  private runBackgroundAnalysisLoop(): void {
    if (!this.backgroundAnalysisActive || !this.backgroundVideoElement) {
      console.log("âš ï¸ Background analysis not active or video element missing");
      return;
    }

    console.log("ğŸ”„ Starting background analysis loop...");

    const processFrame = async () => {
      if (!this.backgroundAnalysisActive || !this.backgroundVideoElement) {
        console.log("ğŸ›‘ Background analysis loop stopped");
        return;
      }

      const now = performance.now();
      const needsProcessing = now - this.lastFrameTime >= this.frameInterval;

      if (needsProcessing && this.isInitialized) {
        console.log("ğŸ”„ Processing frame in background...");
        this.lastFrameTime = now;
        try {
          await this.runAIProcessors(this.backgroundVideoElement, now);
        } catch (e) {
          console.error("âŒ Error in background AI processing:", e);
        }
      }

      this.backgroundAnalysisLoop = requestAnimationFrame(processFrame);
    };

    processFrame();
  }

  public async processVideoTrack(originalTrack: MediaStreamTrack): Promise<MediaStreamTrack> {
    if (originalTrack.kind !== "video") {
      console.warn("Only video tracks can be AI processed.");
      return originalTrack;
    }

    this.stopProcessing();

    this.activeSourceTrack = originalTrack;

    console.log("ğŸ¯ Starting AI video track processing...", {
      trackSettings: originalTrack.getSettings(),
      isInitialized: this.isInitialized
    });

    const videoElem = document.createElement("video");
    videoElem.srcObject = new MediaStream([originalTrack]);
    videoElem.autoplay = true;
    videoElem.muted = true;

    const outputCanvas = document.createElement("canvas");
    outputCanvas.width = originalTrack.getSettings().width || 640;
    outputCanvas.height = originalTrack.getSettings().height || 480;
    const ctx = outputCanvas.getContext("2d");

    return new Promise<MediaStreamTrack>((resolve, reject) => {
      let isResolved = false;
      
      // íƒ€ì„ì•„ì›ƒ ì„¤ì •
      const timeout = setTimeout(() => {
        if (!isResolved) {
          console.error("âŒ AI video track processing timeout");
          reject(new Error("AI video track processing timeout"));
        }
      }, 10000); // 10ì´ˆ íƒ€ì„ì•„ì›ƒ

      videoElem.onloadedmetadata = () => {
        console.log("ğŸ“¹ Video metadata loaded, starting frame processing...");
        
        const processFrame = async () => {
          if (videoElem.paused || videoElem.ended) return;

          const now = performance.now();
          const needsProcessing = now - this.lastFrameTime >= this.frameInterval;

          if (ctx) {
            ctx.drawImage(videoElem, 0, 0, outputCanvas.width, outputCanvas.height);
            if (this.aiConfig.beauty.enabled && this.beautyFilterProcessor) {
              const imageData = ctx.getImageData(0, 0, outputCanvas.width, outputCanvas.height);
              const filteredData = await this.beautyFilterProcessor.applyFilters(imageData);
              ctx.putImageData(filteredData, 0, 0);
            }

            this.renderGestureOverlays(ctx, outputCanvas, now);

            if (
              needsProcessing &&
              this.isInitialized &&
              this.gestureProcessor &&
              this.emotionFaceProcessor
            ) {
              this.lastFrameTime = now;
              try {
                await this.runAIProcessors(videoElem, now);
              } catch (e) {
                console.error("Error processing AI for frame:", e);
              }
            }
          }
          requestAnimationFrame(processFrame);
        };
        
        videoElem.onplaying = () => {
          if (!isResolved) {
            console.log("â–¶ï¸ Video started playing, returning AI-processed track");
            const processedStream = outputCanvas.captureStream(30); // 30 FPS
            const processedTrack = processedStream.getVideoTracks()[0];
            
            if (processedTrack) {
              isResolved = true;
              clearTimeout(timeout);
              console.log("âœ… AI-processed track ready:", {
                trackId: processedTrack.id,
                enabled: processedTrack.enabled,
                readyState: processedTrack.readyState
              });
              this.activeProcessedTrack = processedTrack;
              resolve(processedTrack);
            } else {
              console.error("âŒ Failed to get processed track from canvas stream");
              reject(new Error("Failed to get processed track from canvas stream"));
            }
          }
        };
        
        videoElem.play().then(() => {
          processFrame();
        }).catch((error) => {
          console.error("âŒ Failed to start video playback:", error);
          reject(error);
        });
      };

      videoElem.onerror = (error) => {
        console.error("âŒ Video element error:", error);
        clearTimeout(timeout);
        reject(new Error("Video element error"));
      };
    });
  }

  

  private async runAIProcessors(videoElement: HTMLVideoElement, timestamp: number): Promise<void> {
    if (!this.isInitialized) {
      console.log("âš ï¸ AI processors not initialized");
      return;
    }
    
    console.log("ğŸ¤– Running AI processors...", {
      emotionEnabled: this.aiConfig.emotion.enabled,
      gestureStaticEnabled: this.aiConfig.gesture.static.enabled,
      gestureDynamicEnabled: this.aiConfig.gesture.dynamic.enabled
    });
    
    // ê°ì • ì¸ì‹ ì²˜ë¦¬ (ì†ë„ ì œì–´ ì ìš©)
    if (this.aiConfig.emotion.enabled && this.emotionFaceProcessor) {
      console.log("ğŸ˜Š Processing emotion...");
      const faceCanvas = document.createElement("canvas");
      faceCanvas.width = videoElement.videoWidth;
      faceCanvas.height = videoElement.videoHeight;
      const ctx = faceCanvas.getContext("2d");
      if (!ctx) return;
      ctx.drawImage(videoElement, 0, 0, faceCanvas.width, faceCanvas.height);
      const faceImageData = ctx.getImageData(0, 0, faceCanvas.width, faceCanvas.height);
      const emotionResult = await this.emotionFaceProcessor.detectEmotion(faceImageData, timestamp);
      
      console.log("ğŸ˜Š Emotion result:", emotionResult);
      
      if (emotionResult && this.onEmotionResultCallback) {
        // ê°ì • ê²°ê³¼ í‘œì‹œ ê°„ê²© ì œì–´
        const timeSinceLastEmotion = timestamp - this.lastEmotionResultTime;
        if (timeSinceLastEmotion >= this.EMOTION_RESULT_INTERVAL || emotionResult.label !== "none") {
          console.log("ğŸ˜Š Calling emotion callback...");
          this.onEmotionResultCallback(emotionResult);
          this.lastEmotionResultTime = timestamp;
          // Add emotion overlay
          this.addEmotionOverlay(emotionResult, timestamp);
        }
      }
    }

    // ì œìŠ¤ì²˜ ì¸ì‹ ì²˜ë¦¬ (ì†ë„ ì œì–´ ì ìš©)
    if (
      (this.aiConfig.gesture.static.enabled || this.aiConfig.gesture.dynamic.enabled) &&
      this.gestureProcessor
    ) {
      console.log("ğŸ¤² Processing gestures...");
      const gestureResult = await this.gestureProcessor.detectGestures(videoElement, timestamp);
      
      console.log("ğŸ¤² Gesture result:", gestureResult);
      
      if (gestureResult && this.onGestureResultCallback) {
        // ì œìŠ¤ì²˜ ê²°ê³¼ í‘œì‹œ ê°„ê²© ì œì–´ (noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¦‰ì‹œ í‘œì‹œ)
        const timeSinceLastGesture = timestamp - this.lastGestureResultTime;
        const hasValidGesture = gestureResult.static.label !== "none" || gestureResult.dynamic.label !== "none";
        
        if (timeSinceLastGesture >= this.GESTURE_RESULT_INTERVAL || hasValidGesture) {
          console.log("ğŸ¤² Calling gesture callback...");
          this.onGestureResultCallback(gestureResult);
          this.lastGestureResultTime = timestamp;
          // Add gesture overlay
          this.addGestureOverlay(gestureResult, timestamp);
        }
      }
    }
  }

  private renderGestureOverlays(
    ctx: CanvasRenderingContext2D,
    canvas: HTMLCanvasElement,
    now: number
  ): void {
    this.activeGestureEmojis.forEach((item, key) => {
      const elapsed = now - item.timestamp;
      const remaining = item.duration - elapsed;
      if (remaining > 0) {
        this.updateGestureAnimation(item, elapsed);
        const x = item.x * canvas.width;
        const y = item.y * canvas.height;
        ctx.save();
        ctx.globalAlpha = item.opacity;
        ctx.translate(x, y);
        ctx.scale(item.scale, item.scale);
        ctx.translate(-x, -y);
        const fontSize = Math.min(canvas.width, canvas.height) * 0.08;
        ctx.font = `${fontSize}px Arial`;
        ctx.textAlign = "center";
        ctx.textBaseline = "middle";
        ctx.fillText(item.emoji, x, y);
        ctx.restore();
      } else {
        this.activeGestureEmojis.delete(key);
      }
    });
  }

  private updateGestureAnimation(item: any, elapsed: number): void {
    if (elapsed < this.ANIMATION_FADE_DURATION) {
      const progress = elapsed / this.ANIMATION_FADE_DURATION;
      item.opacity = progress;
      item.scale = 1.2 + 0.3 * (1 - progress);
      item.animationPhase = "fadeIn";
    } else if (elapsed < item.duration - this.ANIMATION_FADE_DURATION) {
      item.opacity = 1.0;
      item.scale = 1.2;
      item.animationPhase = "display";
    } else {
      const fadeOutProgress =
        (elapsed - (item.duration - this.ANIMATION_FADE_DURATION)) / this.ANIMATION_FADE_DURATION;
      item.opacity = 1.0 - fadeOutProgress;
      item.scale = 1.2 - 0.2 * fadeOutProgress;
      item.animationPhase = "fadeOut";
    }
  }

  private addGestureOverlay(gestureResult: GestureResult, timestamp: number): void {
    // Static gesture overlay
    if (gestureResult.static && gestureResult.static.label !== "none" && gestureResult.static.confidence > 0.7) {
      const emoji = this.getGestureEmoji(gestureResult.static.label);
      const key = `static_${gestureResult.static.label}_${timestamp}`;
      this.activeGestureEmojis.set(key, {
        emoji,
        x: 0.3 + Math.random() * 0.4, // Random position
        y: 0.3 + Math.random() * 0.4,
        timestamp,
        duration: this.STATIC_GESTURE_DURATION,
        opacity: 0,
        scale: 5,
        animationPhase: "fadeIn"
      });
    }

    // Dynamic gesture overlay
    if (gestureResult.dynamic && gestureResult.dynamic.label !== "none" && gestureResult.dynamic.confidence > 0.8) {
      const emoji = this.getGestureEmoji(gestureResult.dynamic.label);
      const key = `dynamic_${gestureResult.dynamic.label}_${timestamp}`;
      this.activeGestureEmojis.set(key, {
        emoji,
        x: 0.2 + Math.random() * 0.6,
        y: 0.2 + Math.random() * 0.6,
        timestamp,
        duration: this.DYNAMIC_GESTURE_DURATION,
        opacity: 0,
        scale: 5,
        animationPhase: "fadeIn"
      });
    }
  }

  private addEmotionOverlay(emotionResult: EmotionResult, timestamp: number): void {
    if (emotionResult.label !== "none" && emotionResult.confidence > 0.6) {
      const emoji = this.getEmotionEmoji(emotionResult.label);
      const key = `emotion_${emotionResult.label}_${timestamp}`;
      this.activeGestureEmojis.set(key, {
        emoji,
        x: 0.1 + Math.random() * 0.8,
        y: 0.1 + Math.random() * 0.3, // Top area for emotions
        timestamp,
        duration: 2000, // 2 seconds for emotions
        opacity: 0,
        scale: 3,
        animationPhase: "fadeIn"
      });
    }
  }

  private getGestureEmoji(label: string): string {
    const gestureEmojis: { [key: string]: string } = {
      // Static gestures
      bad: "ğŸ‘",
      fist: "âœŠ", 
      good: "ğŸ‘",
      gun: "ğŸ‘‰",
      heart: "ğŸ«¶",
      ok: "ğŸ‘Œ",
      open_palm: "âœ‹",
      promise: "ğŸ¤™",
      rock: "ğŸ¤˜",
      victory: "âœŒï¸",
      // Dynamic gestures
      fire: "ğŸ”¥",
      hi: "ğŸ‘‹",
      hit: "ğŸ’¥",
      nono: "ğŸš«",
      nyan: "ğŸ¾",
      shot: "ğŸ’–"
    };
    return gestureEmojis[label] || "ğŸ‘Œ";
  }

  private getEmotionEmoji(label: string): string {
    const emotionEmojis: { [key: string]: string } = {
      laugh: "ğŸ˜„",
      serious: "ğŸ˜¤",
      surprise: "ğŸ˜²",
      yawn: "ğŸ¥±",
      angry: "ğŸ˜ ",
      sad: "ğŸ˜¢",
      happy: "ğŸ˜Š"
    };
    return emotionEmojis[label] || "ğŸ˜";
  }

    /**
   * í˜„ì¬ ì§„í–‰ ì¤‘ì¸ AI ì²˜ë¦¬ë¥¼ ì¤‘ì§€í•˜ê³  ê´€ë ¨ íŠ¸ë™ì„ ì •ë¦¬í•˜ëŠ” ìƒˆë¡œìš´ ë©”ì„œë“œ
   */
  public stopProcessing(): void {
    if (this.activeSourceTrack) {
      this.activeSourceTrack.stop();
      this.activeSourceTrack = null;
      console.log("ğŸ›‘ Stopped previous AI source track.");
    }
    if (this.activeProcessedTrack) {
      this.activeProcessedTrack.stop();
      this.activeProcessedTrack = null;
      console.log("ğŸ›‘ Stopped previous AI processed track.");
    }
    this.stopBackgroundAnalysis(); // ê¸°ì¡´ ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì¤‘ì§€ ë¡œì§ë„ í˜¸ì¶œ
  }

  public cleanup(): void {
    this.stopProcessing(); 
    // ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì¤‘ì§€
    this.stopBackgroundAnalysis();
    
    emotionCaptureManager.cleanup();
    this.dispatch = null;
    this.activeGestureEmojis.clear();
    this.onGestureResultCallback = null;
    this.onEmotionResultCallback = null;
    this.isInitialized = false;
    
    // íƒ€ì´ë° ê´€ë ¨ ë³€ìˆ˜ ë¦¬ì…‹
    this.lastFrameTime = 0;
    this.lastGestureResultTime = 0;
    this.lastEmotionResultTime = 0;
    
    this.emotionFaceProcessor?.cleanup();
    this.emotionFaceProcessor = null;
    this.beautyFilterProcessor?.cleanup();
    this.beautyFilterProcessor = null;
    this.gestureProcessor?.cleanup();
    this.gestureProcessor = null;
    console.log("FrontendAiProcessor cleaned up.");
  }
}

export const frontendAiProcessor = new FrontendAiProcessor();
