// src/shared/api/ai/frontendAiProcessor.ts

import { AppDispatch } from "@/shared/config/store";
import {
  GestureResult,
  EmotionResult,
  AiSystemConfig,
  GestureCallback,
  EmotionCallback,
} from "@/shared/types/ai.types";

import { emotionCaptureManager } from "./emotionCaptureManager";
import { EmotionFaceProcessor } from "./emotionFaceProcessor";
import { BeautyFilterProcessor } from "./beautyFilterProcessor";
import { GestureProcessor } from "./gestureProcessor";
import * as tf from "@tensorflow/tfjs";

// --- AI ëª¨ë¸ ê²½ë¡œ ìƒìˆ˜ ì •ì˜ ---
const MODELS_BASE_PATH = "/models";

// í‘œì • ì¸ì‹ ëª¨ë¸ ê²½ë¡œ
const EXPRESSION_MODEL_PATH = `${MODELS_BASE_PATH}/expression/model.json`;
const EXPRESSION_SCALER_PATH = `${MODELS_BASE_PATH}/expression/scaler_v3.json`;

// ì œìŠ¤ì²˜ ì¸ì‹ ëª¨ë¸ ê²½ë¡œ
const STATIC_GESTURE_MODEL_PATH = `${MODELS_BASE_PATH}/static-gesture/model.json`;
const DYNAMIC_GESTURE_MODEL_PATH = `${MODELS_BASE_PATH}/dinamic-gesture/model.json`;

// MediaPipe WASM íŒŒì¼ CDN ê²½ë¡œ
const FACE_MESH_WASM_PATH = "https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh";
const TASKS_VISION_WASM_PATH = "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm";

// âœ¨ ì´ë¯¸ì§€ ì˜¤ë²„ë ˆì´ë¥¼ ìœ„í•œ ê²½ë¡œ ìƒìˆ˜ ì¶”ê°€
const STATIC_IMAGE_BASE_PATH = "/images/gestures/static";
const DYNAMIC_IMAGE_BASE_PATH = "/images/gestures/dynamic";
const EMOTION_IMAGE_BASE_PATH = "/images/gestures/emotion";

class FrontendAiProcessor {
  private dispatch: AppDispatch | null = null;
  private aiConfig: AiSystemConfig = {
    gesture: {
      static: { enabled: true, confidence: 0.75 },
      dynamic: { enabled: true, confidence: 0.9 },
    },
    emotion: { enabled: false, confidence: 0.6 },
    beauty: { enabled: false, gamma: 1.4, lipAlpha: 0.2, smoothAmount: 30, lipColor: [255, 0, 0] },
  };

  private emotionFaceProcessor: EmotionFaceProcessor | null = null;
  private beautyFilterProcessor: BeautyFilterProcessor | null = null;
  private gestureProcessor: GestureProcessor | null = null;
  private isInitialized = false;

  private onGestureResultCallback: GestureCallback | null = null;
  private onEmotionResultCallback: EmotionCallback | null = null;

  private lastFrameTime = 0;
  private frameInterval = 100;
  
  // ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ìš© ë³€ìˆ˜ë“¤
  private backgroundVideoElement: HTMLVideoElement | null = null;
  private backgroundAnalysisActive = false;
  private backgroundAnalysisLoop: number | null = null;
  
  // AI ê²°ê³¼ í‘œì‹œ ì†ë„ ì œì–´
  private lastGestureResultTime = 0;
  private lastEmotionResultTime = 0;
  private readonly GESTURE_RESULT_INTERVAL = 1500;
  private readonly EMOTION_RESULT_INTERVAL = 5000; // ê°ì • ì¸ì‹ ê°„ê²©ì„ 5ì´ˆë¡œ ì¦ê°€

  // âœ¨ ì˜¤ë²„ë ˆì´ (ì´ë¯¸ì§€ í¬í•¨)
  private activeOverlays: Map<string, {
    image: HTMLImageElement;
    x: number;
    y: number;
    timestamp: number;
    duration: number;
    opacity: number;
    scale: number;
  }> = new Map();

  private readonly STATIC_GESTURE_DURATION = 1500;
  private readonly DYNAMIC_GESTURE_DURATION = 1500;
  private readonly ANIMATION_FADE_DURATION = 150;

  private activeSourceTrack: MediaStreamTrack | null = null;
  private activeProcessedTrack: MediaStreamTrack | null = null;

  // âœ¨ ë¡œë“œëœ ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•  Map ê°ì²´
  private loadedImages: Map<string, HTMLImageElement> = new Map();

  // âœ¨ ğŸš¨ ìˆ˜ì •: app.py STATIC_IMG_MAPê³¼ ì¼ì¹˜í•˜ë„ë¡ ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì •
  private async preloadImages(): Promise<void> {
    const imagePaths: { [key: string]: string } = {
      // Static gestures - app.py STATIC_IMG_MAPê³¼ ì •í™•íˆ ì¼ì¹˜
      // ğŸš¨ ì¤‘ìš”: fistì™€ open_palm ì œê±° (app.pyì—ì„œ ì˜¤ë²„ë ˆì´ë˜ì§€ ì•ŠìŒ)
      bad: `${STATIC_IMAGE_BASE_PATH}/bad.png`,
      good: `${STATIC_IMAGE_BASE_PATH}/good.png`,
      gun: `${STATIC_IMAGE_BASE_PATH}/gun.png`,
      heart: `${STATIC_IMAGE_BASE_PATH}/heart.png`,
      ok: `${STATIC_IMAGE_BASE_PATH}/ok.png`,
      promise: `${STATIC_IMAGE_BASE_PATH}/promise.png`,
      rock: `${STATIC_IMAGE_BASE_PATH}/rock.png`,
      victory: `${STATIC_IMAGE_BASE_PATH}/victory.png`,
      
      // Dynamic gestures - app.py DYN_LABELSì™€ ì¼ì¹˜
      fire: `${DYNAMIC_IMAGE_BASE_PATH}/fire.png`,
      hi: `${DYNAMIC_IMAGE_BASE_PATH}/hi.png`,
      hit: `${DYNAMIC_IMAGE_BASE_PATH}/hit.png`,
      nono: `${DYNAMIC_IMAGE_BASE_PATH}/nono.png`,
      nyan: `${DYNAMIC_IMAGE_BASE_PATH}/nyan.png`,
      shot: `${DYNAMIC_IMAGE_BASE_PATH}/shot.png`,
      
      // Emotions
      laugh: `${EMOTION_IMAGE_BASE_PATH}/laugh.png`,
      serious: `${EMOTION_IMAGE_BASE_PATH}/serious.png`,
      surprise: `${EMOTION_IMAGE_BASE_PATH}/surprise.png`,
      yawn: `${EMOTION_IMAGE_BASE_PATH}/yawn.png`,
    };

    const promises = Object.entries(imagePaths).map(([key, src]) => {
      return new Promise<void>((resolve) => {
        const img = new Image();
        img.src = src;
        img.onload = () => {
          this.loadedImages.set(key, img);
          resolve();
        };
        img.onerror = () => {
          console.error(`âŒ Failed to load image: ${src}`);
          resolve(); // ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        };
      });
    });

    await Promise.all(promises);
    console.log("âœ… All overlay images preloaded.");
  }
  
  public async init(dispatch: AppDispatch): Promise<void> {
    this.dispatch = dispatch;
    emotionCaptureManager.init(dispatch);
    
    try {
      await tf.setBackend('webgl');
      console.log(`TensorFlow.js backend set to: ${tf.getBackend()}`);
    } catch (error) {
      console.warn("âŒ Failed to set TensorFlow.js WebGL backend, falling back to CPU:", error);
    }

    this.emotionFaceProcessor = new EmotionFaceProcessor(this.aiConfig);
    this.beautyFilterProcessor = new BeautyFilterProcessor(this.aiConfig);
    this.gestureProcessor = new GestureProcessor(this.aiConfig);

    try {
      console.log("ğŸ¤– Initializing AI modules and preloading images...");
      console.log(`- Emotion model: ${EXPRESSION_MODEL_PATH}`);
      console.log(`- Emotion scaler: ${EXPRESSION_SCALER_PATH}`);
      
      // âœ¨ AI ëª¨ë¸ ì´ˆê¸°í™”ì™€ ë™ì‹œì— ì´ë¯¸ì§€ ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤í–‰
      await Promise.all([
        this.preloadImages(),
        this.emotionFaceProcessor.init(
          EXPRESSION_MODEL_PATH,
          EXPRESSION_SCALER_PATH,
          FACE_MESH_WASM_PATH
        ),
        this.beautyFilterProcessor.init(FACE_MESH_WASM_PATH),
        this.gestureProcessor.init(TASKS_VISION_WASM_PATH),
      ]);

      console.log("ğŸ”„ Loading gesture models...");
      console.log(`- Static gesture model: ${STATIC_GESTURE_MODEL_PATH}`);
      console.log(`- Dynamic gesture model: ${DYNAMIC_GESTURE_MODEL_PATH}`);

      this.isInitialized = true;
      console.log("âœ… FrontendAiProcessor initialized successfully.");
      
    } catch (error) {
      console.error("âŒ FrontendAiProcessor: Failed to initialize AI models:", error);
      this.isInitialized = false;
    }
  }


  public updateConfig(config: Partial<AiSystemConfig>): Promise<void> {
    return Promise.resolve().then(() => {
      this.aiConfig = {
        ...this.aiConfig,
        ...config,
        gesture: { ...this.aiConfig.gesture, ...config.gesture },
        emotion: { ...this.aiConfig.emotion, ...config.emotion },
        beauty: { ...this.aiConfig.beauty, ...config.beauty },
      };

      this.emotionFaceProcessor?.updateConfig(this.aiConfig);
      this.beautyFilterProcessor?.updateConfig(this.aiConfig);
      this.gestureProcessor?.updateConfig(this.aiConfig);
    });
  }

  public setGestureCallback(callback: GestureCallback): void {
    this.onGestureResultCallback = callback;
  }

  public setEmotionCallback(callback: EmotionCallback): void {
    this.onEmotionResultCallback = callback;
  }

  public startBackgroundAnalysis(originalTrack: MediaStreamTrack): void {
    if (originalTrack.kind !== "video") {
      console.warn("Only video tracks can be analyzed.");
      return;
    }

    console.log("ğŸ¤– Starting background AI analysis...");
    this.stopBackgroundAnalysis();

    this.backgroundVideoElement = document.createElement("video");
    this.backgroundVideoElement.srcObject = new MediaStream([originalTrack]);
    this.backgroundVideoElement.autoplay = true;
    this.backgroundVideoElement.muted = true;
    this.backgroundVideoElement.style.display = "none";

    this.backgroundVideoElement.onloadedmetadata = () => {
      this.backgroundAnalysisActive = true;
      this.runBackgroundAnalysisLoop();
      console.log("âœ… Background AI analysis started");
    };

    this.backgroundVideoElement.play();
  }

  public stopBackgroundAnalysis(): void {
    console.log("ğŸ›‘ Stopping background AI analysis...");
    this.backgroundAnalysisActive = false;
    
    if (this.backgroundAnalysisLoop) {
      cancelAnimationFrame(this.backgroundAnalysisLoop);
      this.backgroundAnalysisLoop = null;
    }

    if (this.backgroundVideoElement) {
      this.backgroundVideoElement.pause();
      this.backgroundVideoElement.srcObject = null;
      this.backgroundVideoElement = null;
    }

    console.log("âœ… Background AI analysis stopped");
  }

  private runBackgroundAnalysisLoop(): void {
    if (!this.backgroundAnalysisActive || !this.backgroundVideoElement) {
      console.log("âš ï¸ Background analysis not active or video element missing");
      return;
    }

    console.log("ğŸ”„ Starting background analysis loop...");

    const processFrame = async () => {
      if (!this.backgroundAnalysisActive || !this.backgroundVideoElement) {
        console.log("ğŸ›‘ Background analysis loop stopped");
        return;
      }

      const now = performance.now();
      const needsProcessing = now - this.lastFrameTime >= this.frameInterval;

      if (needsProcessing && this.isInitialized) {
        console.log("ğŸ”„ Processing frame in background...");
        this.lastFrameTime = now;
        try {
          await this.runAIProcessors(this.backgroundVideoElement, now);
        } catch (e) {
          console.error("âŒ Error in background AI processing:", e);
        }
      }

      this.backgroundAnalysisLoop = requestAnimationFrame(processFrame);
    };

    processFrame();
  }

  public async processVideoTrack(originalTrack: MediaStreamTrack): Promise<MediaStreamTrack> {
    if (originalTrack.kind !== "video") {
      console.warn("Only video tracks can be AI processed.");
      return originalTrack;
    }

    this.stopProcessing();
    this.activeSourceTrack = originalTrack;

    console.log("ğŸ¯ Starting AI video track processing...", {
      trackSettings: originalTrack.getSettings(),
      isInitialized: this.isInitialized
    });

    const videoElem = document.createElement("video");
    videoElem.srcObject = new MediaStream([originalTrack]);
    videoElem.autoplay = true;
    videoElem.muted = true;

    const outputCanvas = document.createElement("canvas");
    outputCanvas.width = originalTrack.getSettings().width || 640;
    outputCanvas.height = originalTrack.getSettings().height || 480;
    const ctx = outputCanvas.getContext("2d");

    return new Promise<MediaStreamTrack>((resolve, reject) => {
      let isResolved = false;
      
      const timeout = setTimeout(() => {
        if (!isResolved) {
          console.error("âŒ AI video track processing timeout");
          reject(new Error("AI video track processing timeout"));
        }
      }, 10000);

      videoElem.onloadedmetadata = () => {
        console.log("ğŸ“¹ Video metadata loaded, starting frame processing...");
        
        const processFrame = async () => {
          if (videoElem.paused || videoElem.ended) return;

          const now = performance.now();
          const needsProcessing = now - this.lastFrameTime >= this.frameInterval;

          if (ctx) {
            ctx.drawImage(videoElem, 0, 0, outputCanvas.width, outputCanvas.height);
            if (this.aiConfig.beauty.enabled && this.beautyFilterProcessor) {
              const imageData = ctx.getImageData(0, 0, outputCanvas.width, outputCanvas.height);
              const filteredData = await this.beautyFilterProcessor.applyFilters(imageData);
              ctx.putImageData(filteredData, 0, 0);
            }

            // âœ¨ ì˜¤ë²„ë ˆì´ ë Œë”ë§
            this.renderOverlays(ctx, outputCanvas, now);

            if (
              needsProcessing &&
              this.isInitialized &&
              this.gestureProcessor &&
              this.emotionFaceProcessor
            ) {
              this.lastFrameTime = now;
              try {
                await this.runAIProcessors(videoElem, now);
              } catch (e) {
                console.error("Error processing AI for frame:", e);
              }
            }
          }
          requestAnimationFrame(processFrame);
        };
        
        videoElem.onplaying = () => {
          if (!isResolved) {
            console.log("â–¶ï¸ Video started playing, returning AI-processed track");
            const processedStream = outputCanvas.captureStream(30);
            const processedTrack = processedStream.getVideoTracks()[0];
            
            if (processedTrack) {
              isResolved = true;
              clearTimeout(timeout);
              console.log("âœ… AI-processed track ready:", {
                trackId: processedTrack.id,
                enabled: processedTrack.enabled,
                readyState: processedTrack.readyState
              });
              this.activeProcessedTrack = processedTrack;
              resolve(processedTrack);
            } else {
              console.error("âŒ Failed to get processed track from canvas stream");
              reject(new Error("Failed to get processed track from canvas stream"));
            }
          }
        };
        
        videoElem.play().then(() => {
          processFrame();
        }).catch((error) => {
          console.error("âŒ Failed to start video playback:", error);
          reject(error);
        });
      };

      videoElem.onerror = (error) => {
        console.error("âŒ Video element error:", error);
        clearTimeout(timeout);
        reject(new Error("Video element error"));
      };
    });
  }

  private async runAIProcessors(videoElement: HTMLVideoElement, timestamp: number): Promise<void> {
    if (!this.isInitialized) {
      console.log("âš ï¸ AI processors not initialized");
      return;
    }
    
    // ê°ì • ì¸ì‹ ì²˜ë¦¬
    if (this.aiConfig.emotion.enabled && this.emotionFaceProcessor) {
      const faceCanvas = document.createElement("canvas");
      faceCanvas.width = videoElement.videoWidth;
      faceCanvas.height = videoElement.videoHeight;
      const ctx = faceCanvas.getContext("2d");
      if (!ctx) return;
      ctx.drawImage(videoElement, 0, 0, faceCanvas.width, faceCanvas.height);
      const faceImageData = ctx.getImageData(0, 0, faceCanvas.width, faceCanvas.height);
      const emotionResult = await this.emotionFaceProcessor.detectEmotion(faceImageData, timestamp);
      
      if (emotionResult && this.onEmotionResultCallback) {
        const timeSinceLastEmotion = timestamp - this.lastEmotionResultTime;
        if (timeSinceLastEmotion >= this.EMOTION_RESULT_INTERVAL || emotionResult.label !== "none") {
          this.onEmotionResultCallback(emotionResult);
          this.lastEmotionResultTime = timestamp;
          this.addEmotionOverlay(emotionResult, timestamp);
        }
      }
    }

    // ì œìŠ¤ì²˜ ì¸ì‹ ì²˜ë¦¬
    if (
      (this.aiConfig.gesture.static.enabled || this.aiConfig.gesture.dynamic.enabled) &&
      this.gestureProcessor
    ) {
      const gestureResult = await this.gestureProcessor.detectGestures(videoElement, timestamp);
      
      if (gestureResult && this.onGestureResultCallback) {
        const timeSinceLastGesture = timestamp - this.lastGestureResultTime;
        const hasValidGesture = gestureResult.static.label !== "none" || gestureResult.dynamic.label !== "none";
        
        if (timeSinceLastGesture >= this.GESTURE_RESULT_INTERVAL || hasValidGesture) {
          this.onGestureResultCallback(gestureResult);
          this.lastGestureResultTime = timestamp;
          this.addGestureOverlay(gestureResult, timestamp);
        }
      }
    }
  }

  // âœ¨ ë Œë”ë§ í•¨ìˆ˜ ìˆ˜ì •: drawImage ì‚¬ìš©
  private renderOverlays(
    ctx: CanvasRenderingContext2D,
    canvas: HTMLCanvasElement,
    now: number
  ): void {
    this.activeOverlays.forEach((item, key) => {
      const elapsed = now - item.timestamp;
      const remaining = item.duration - elapsed;
      if (remaining > 0) {
        this.updateGestureAnimation(item, elapsed);
        
        const x = item.x * canvas.width;
        const y = item.y * canvas.height;
        
        const baseSize = Math.min(canvas.width, canvas.height) * 0.3;
        const imgWidth = baseSize * item.scale;
        const imgHeight = (baseSize * (item.image.height / item.image.width)) * item.scale;

        ctx.save();
        ctx.globalAlpha = item.opacity;
        
        // âœ¨ drawImageë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë Œë”ë§
        ctx.drawImage(
          item.image,
          x - imgWidth / 2, // ì¤‘ì•™ ì •ë ¬
          y - imgHeight / 2,
          imgWidth,
          imgHeight
        );
        
        ctx.restore();
      } else {
        this.activeOverlays.delete(key);
      }
    });
  }

  private updateGestureAnimation(item: {
    image: HTMLImageElement;
    x: number;
    y: number;
    timestamp: number;
    duration: number;
    opacity: number;
    scale: number;
  }, elapsed: number): void {
    const fadeInDuration = this.ANIMATION_FADE_DURATION;
    const fadeOutDuration = this.ANIMATION_FADE_DURATION;
    const totalDuration = item.duration;

    if (elapsed < fadeInDuration) {
        // Fade In
        const progress = elapsed / fadeInDuration;
        item.opacity = progress;
        item.scale = 1.0 + 0.5 * (1 - progress); // ì‹œì‘í•  ë•Œ í¬ê²Œ
    } else if (elapsed < totalDuration - fadeOutDuration) {
        // Display
        item.opacity = 1.0;
        item.scale = 1.0;
    } else {
        // Fade Out
        const fadeOutElapsed = elapsed - (totalDuration - fadeOutDuration);
        const progress = fadeOutElapsed / fadeOutDuration;
        item.opacity = 1.0 - progress;
        item.scale = 1.0 - 0.2 * progress; // ì‚¬ë¼ì§€ë©´ì„œ ì‘ê²Œ
    }
  }

  // âœ¨ ìˆ˜ì •: ì‹¤ì œ ì† ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•œ ì œìŠ¤ì²˜ ì˜¤ë²„ë ˆì´
  private addGestureOverlay(gestureResult: GestureResult, timestamp: number): void {
    const processGesture = (
        gesture: { label: string; confidence: number } | null,
        type: 'static' | 'dynamic'
    ) => {
        // shotì€ íŠ¹ë³„íˆ ë†’ì€ ì„ê³„ê°’, ì •ì  ì œìŠ¤ì²˜ë„ ì¡°ê¸ˆ ë†’ì„
        const confidenceThreshold = gesture?.label === 'shot' ? 0.98 : (type === 'static' ? 0.8 : 0.85);
        if (!gesture || gesture.label === "none" || gesture.confidence < confidenceThreshold) return;

        const image = this.getImageForLabel(gesture.label);
        if (!image) return; // ğŸš¨ app.pyì²˜ëŸ¼ ì´ë¯¸ì§€ ì—†ìœ¼ë©´ ì˜¤ë²„ë ˆì´ ì•ˆ í•¨

        // âœ¨ ì‹¤ì œ ì† ìœ„ì¹˜ ì¶”ì¶œ
        let handX = 0.5; // ê¸°ë³¸ê°’ (ì¤‘ì•™)
        let handY = 0.5; // ê¸°ë³¸ê°’ (ì¤‘ì•™)
        
        console.log(`ğŸ–ï¸ [${type}] ${gesture.label} - landmarks:`, gestureResult.landmarks?.length || 0);
        
        if (gestureResult.landmarks && gestureResult.landmarks.length > 0) {
          // ì²« ë²ˆì§¸ ì†ì˜ ì†ëª© ì¢Œí‘œ (ëœë“œë§ˆí¬ 0ë²ˆ)ë¥¼ ì‚¬ìš©
          // landmarksëŠ” [hand1_landmark0, hand1_landmark1, ...] í˜•íƒœ
          // ê° ëœë“œë§ˆí¬ëŠ” [x, y, z] ë°°ì—´
          const wristLandmark = gestureResult.landmarks[0]; // ì†ëª© (landmark 0)
          console.log(`ğŸ¯ ì†ëª© ëœë“œë§ˆí¬:`, wristLandmark);
          
          if (wristLandmark && wristLandmark.length >= 2) {
            handX = wristLandmark[0]; // ì •ê·œí™”ëœ x ì¢Œí‘œ (0-1)
            handY = wristLandmark[1]; // ì •ê·œí™”ëœ y ì¢Œí‘œ (0-1)
            
            console.log(`ğŸ“ ì›ë³¸ ì† ìœ„ì¹˜: (${handX.toFixed(3)}, ${handY.toFixed(3)})`);
            
            // ì†ëª©ì—ì„œ ì† ìœ„ìª½ìœ¼ë¡œ ì˜¤ë²„ë ˆì´ ìœ„ì¹˜ ì¡°ì • (Yì¶• ìœ„ë¡œ ì´ë™)
            handY = handY - 0.15; // ì†ëª©ì—ì„œ ìœ„ë¡œ 15% ì˜¬ë¦¬ê¸°
            
            // í™”ë©´ ê²½ê³„ ì²´í¬ ë° ë³´ì • (ì˜¤ë²„ë ˆì´ê°€ í™”ë©´ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šë„ë¡)
            const margin = 0.1; // 10% ì—¬ë°±
            handX = Math.max(margin, Math.min(1 - margin, handX));
            handY = Math.max(margin, Math.min(1 - margin, handY));
            
            console.log(`ğŸ¯ ìµœì¢… ì˜¤ë²„ë ˆì´ ìœ„ì¹˜: (${handX.toFixed(3)}, ${handY.toFixed(3)})`);
          } else {
            console.warn(`âš ï¸ ì†ëª© ëœë“œë§ˆí¬ ë°ì´í„° ë¶€ì¡±:`, wristLandmark);
          }
        } else {
          console.warn(`âš ï¸ ëœë“œë§ˆí¬ ë°ì´í„° ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: (${handX}, ${handY})`);
        }

        const key = `${type}_${gesture.label}_${timestamp}`;
        this.activeOverlays.set(key, {
            image,
            x: handX, // ì‹¤ì œ ì† ìœ„ì¹˜ ì‚¬ìš©
            y: handY, // ì‹¤ì œ ì† ìœ„ì¹˜ ì‚¬ìš©
            timestamp,
            duration: type === 'static' ? this.STATIC_GESTURE_DURATION : this.DYNAMIC_GESTURE_DURATION,
            opacity: 0,
            scale: 5,
        });
    };

    if (this.aiConfig.gesture.static.enabled) processGesture(gestureResult.static, 'static');
    if (this.aiConfig.gesture.dynamic.enabled) processGesture(gestureResult.dynamic, 'dynamic');
  }

  private addEmotionOverlay(emotionResult: EmotionResult, timestamp: number): void {
    if (emotionResult.label === "none" || emotionResult.confidence < 0.8) return; // ì‹ ë¢°ë„ ì„ê³„ê°’ì„ 0.8ë¡œ ìƒìŠ¹

    const image = this.getImageForLabel(emotionResult.label);
    if (!image) return;

    // ì–¼êµ´ ìœ„ì¹˜ ê¸°ë³¸ê°’ (í™”ë©´ ì¤‘ì•™ ìƒë‹¨)
    let faceX = 0.5; // í™”ë©´ ì¤‘ì•™
    let faceY = 0.3; // í™”ë©´ ìƒë‹¨ 30% ì§€ì 
    
    // ì–¼êµ´ ëœë“œë§ˆí¬ê°€ ìˆìœ¼ë©´ ì‹¤ì œ ì–¼êµ´ ìœ„ì¹˜ ì‚¬ìš©
    if (emotionResult.faceLandmarks && emotionResult.faceLandmarks.length > 0) {
      // ì–¼êµ´ ëœë“œë§ˆí¬ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
      const landmarks = emotionResult.faceLandmarks;
      const avgX = landmarks.reduce((sum, lm) => sum + lm[0], 0) / landmarks.length;
      const avgY = landmarks.reduce((sum, lm) => sum + lm[1], 0) / landmarks.length;
      
      faceX = avgX;
      faceY = avgY - 0.1; // ì–¼êµ´ ìœ„ìª½ì— í‘œì‹œ
      
      // í™”ë©´ ê²½ê³„ ì²´í¬
      const margin = 0.1;
      faceX = Math.max(margin, Math.min(1 - margin, faceX));
      faceY = Math.max(margin, Math.min(1 - margin, faceY));
    }

    const key = `emotion_${emotionResult.label}_${timestamp}`;
    this.activeOverlays.set(key, {
        image,
        x: faceX,
        y: faceY,
        timestamp,
        duration: 2000,
        opacity: 0,
        scale: 5,
    });
  }

  // âœ¨ ğŸš¨ ìˆ˜ì •: app.pyì™€ ë™ì¼í•˜ê²Œ fist/open_palmì€ null ë°˜í™˜
  private getImageForLabel(label: string): HTMLImageElement | null {
    // app.pyì˜ STATIC_IMG_MAPì— ì—†ëŠ” ì œìŠ¤ì²˜ë“¤ì€ ì˜¤ë²„ë ˆì´í•˜ì§€ ì•ŠìŒ
    if (label === 'fist' || label === 'open_palm') {
      console.log(`ğŸš¨ ${label} detected but no overlay image (matches app.py behavior)`);
      return null;
    }
    
    return this.loadedImages.get(label) || null;
  }

  public stopProcessing(): void {
    // ğŸš¨ ì¤‘ìš”: ì›ë³¸ íŠ¸ë™(activeSourceTrack)ì€ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ!
    // ì›ë³¸ íŠ¸ë™ì€ ì‚¬ìš©ìì˜ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ì´ë¯€ë¡œ AI ì²˜ë¦¬ ì¢…ë£Œì™€ ë¬´ê´€í•˜ê²Œ ìœ ì§€ë˜ì–´ì•¼ í•¨
    if (this.activeSourceTrack) {
      console.log("ğŸ“Œ Releasing reference to source track (not stopping):", this.activeSourceTrack.id);
      this.activeSourceTrack = null; // ì°¸ì¡°ë§Œ í•´ì œ
    }
    
    // AI ì²˜ë¦¬ëœ íŠ¸ë™ë§Œ ì¤‘ë‹¨
    if (this.activeProcessedTrack) {
      this.activeProcessedTrack.stop();
      this.activeProcessedTrack = null;
      console.log("ğŸ›‘ Stopped AI processed track.");
    }
    
    this.stopBackgroundAnalysis();
  }

  public cleanup(): void {
    this.stopProcessing(); 
    this.stopBackgroundAnalysis();
    
    emotionCaptureManager.cleanup();
    this.dispatch = null;
    
    this.activeOverlays.clear();
    
    this.onGestureResultCallback = null;
    this.onEmotionResultCallback = null;
    this.isInitialized = false;
    
    this.lastFrameTime = 0;
    this.lastGestureResultTime = 0;
    this.lastEmotionResultTime = 0;
    
    this.emotionFaceProcessor?.cleanup();
    this.emotionFaceProcessor = null;
    this.beautyFilterProcessor?.cleanup();
    this.beautyFilterProcessor = null;
    this.gestureProcessor?.cleanup();
    this.gestureProcessor = null;
    
    this.loadedImages.clear();

    console.log("FrontendAiProcessor cleaned up.");
  }

  // isInitialized ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” public getter
  public get initialized(): boolean {
    return this.isInitialized;
  }
}

export const frontendAiProcessor = new FrontendAiProcessor();