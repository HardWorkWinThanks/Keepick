// src/widgets/video-conference/ui/GestureRecognizer.tsx
"use client";

import { useEffect, useRef, useState } from "react";
import * as tf from "@tensorflow/tfjs";
import { HandLandmarker, FilesetResolver } from "@mediapipe/tasks-vision";
import type {
  HandLandmarkerResult,
  NormalizedLandmark,
} from "@mediapipe/tasks-vision";

// --- ì„¤ì • ìƒìˆ˜ ---
const SEQUENCE_LENGTH = 30; // ë™ì  ëª¨ë¸ì´ ì‚¬ìš©í•  í”„ë ˆì„ ì‹œí€€ìŠ¤ ê¸¸ì´
const CONFIDENCE_THRESHOLD = 0.8; // ë™ì  ëª¨ë¸ ì˜ˆì¸¡ ì‹ ë¢°ë„ ì„ê³„ê°’

// --- ë ˆì´ë¸” ì •ì˜ ---
// ì •ì  ëª¨ë¸ ë ˆì´ë¸” (ê¸°ì¡´)
const STATIC_LABELS = [
  "bad",
  "fist",
  "good",
  "gun",
  "heart",
  "none",
  "ok",
  "open_palm",
  "promise",
  "rock",
  "victory",
];
const KOREAN_STATIC_LABELS: { [key: string]: string } = {
  bad: "ğŸ‘",
  fist: "ì£¼ë¨¹",
  good: "ğŸ‘",
  gun: "ì´ ëª¨ì–‘",
  heart: "ì†ê°€ë½ í•˜íŠ¸",
  none: "ì—†ìŒ",
  ok: "OK",
  open_palm: "ì†ë°”ë‹¥",
  promise: "ì•½ì†",
  rock: "ë½ì•¤ë¡¤",
  victory: "ë¸Œì´",
};

// ë™ì  ëª¨ë¸ ë ˆì´ë¸” (ì‹ ê·œ)
const DYNAMIC_LABELS = ["fire", "hi", "hit", "none", "nono", "nyan", "shot"];
const KOREAN_DYNAMIC_LABELS: { [key: string]: string } = {
  fire: "ğŸ”¥ íŒŒì´ì–´",
  hi: "ğŸ‘‹ ì•ˆë…•",
  hit: "ğŸ’¥ íˆíŠ¸",
  none: "ì—†ìŒ",
  nono: "ğŸš« ì•ˆë¼",
  nyan: "ğŸ¾ ëƒ¥ëƒ¥í€ì¹˜",
  shot: "ğŸ’– ìƒ·",
};

// ì»´í¬ë„ŒíŠ¸ Props íƒ€ì… ì •ì˜
interface GestureRecognizerProps {
  mediaStream: MediaStream | null;
}

export const GestureRecognizer: React.FC<GestureRecognizerProps> = ({
  mediaStream,
}) => {
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);

  // --- ëª¨ë¸ ë° ë°ì´í„° Ref ---
  const staticModelRef = useRef<tf.LayersModel | null>(null);
  const dynamicModelRef = useRef<tf.LayersModel | null>(null);
  const handLandmarkerRef = useRef<HandLandmarker | null>(null);
  const sequenceRef = useRef<number[][]>([]); // ë™ì  ì¸ì‹ì„ ìœ„í•œ í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤

  // --- UI ìƒíƒœ ---
  const [staticGesture, setStaticGesture] =
    useState<string>("ì •ì : ì¤€ë¹„ ì¤‘...");
  const [dynamicGesture, setDynamicGesture] =
    useState<string>("ë™ì : ì¤€ë¹„ ì¤‘...");

  const animationFrameId = useRef<number | null>(null);
  const lastVideoTimeRef = useRef<number>(-1);

  // 1. ì´ˆê¸°í™” (ëª¨ë¸ ë¡œë”© ë° MediaPipe ì„¤ì •)
  // 1. ì´ˆê¸°í™”: ë‘ ëª¨ë¸ê³¼ MediaPipeë¥¼ í•¨ê»˜ ë¡œë“œ
  useEffect(() => {
    async function setupAllModels() {
      try {
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
        );
        const handLandmarker = await HandLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task`,
            delegate: "GPU",
          },
          runningMode: "VIDEO",
          numHands: 1,
        });
        handLandmarkerRef.current = handLandmarker;

        // Promise.allì„ ì‚¬ìš©í•˜ì—¬ ë‘ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ë¡œë”©
        const [staticModel, dynamicModel] = await Promise.all([
          tf.loadLayersModel("/static_model/model.json"), // ì •ì  ëª¨ë¸ ê²½ë¡œ ìˆ˜ì •
          tf.loadLayersModel("/dynamic_model/model.json"), // ë™ì  ëª¨ë¸ ê²½ë¡œ í™•ì¸
        ]);
        staticModelRef.current = staticModel;
        dynamicModelRef.current = dynamicModel;

        setStaticGesture("ì •ì : ì¸ì‹ ì¤€ë¹„ ì™„ë£Œ");
        setDynamicGesture("ë™ì : ì›€ì§ì„ì„ ë³´ì—¬ì£¼ì„¸ìš”");
      } catch (error) {
        console.error("AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
        setStaticGesture("ì •ì : ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨");
        setDynamicGesture("ë™ì : ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨");
      }
    }
    setupAllModels();

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ìì› ì •ë¦¬
    return () => {
      if (animationFrameId.current) {
        cancelAnimationFrame(animationFrameId.current);
      }
      handLandmarkerRef.current?.close();
      staticModelRef.current?.dispose();
      dynamicModelRef.current?.dispose();
    };
  }, []);

  // 2. MediaStreamì´ ë³€ê²½ë  ë•Œ ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ì— ì—°ê²°
  useEffect(() => {
    if (mediaStream && videoRef.current) {
      videoRef.current.srcObject = mediaStream;
      videoRef.current.addEventListener("loadeddata", () => {
        // ë¹„ë””ì˜¤ ì¬ìƒì´ ì‹œì‘ë˜ë©´ ì˜ˆì¸¡ ë£¨í”„ë¥¼ ì‹œì‘
        startPredictionLoop();
      });
    }

    return () => {
      // ìŠ¤íŠ¸ë¦¼ì´ ë°”ë€Œê±°ë‚˜ ì»´í¬ë„ŒíŠ¸ê°€ ì‚¬ë¼ì§ˆ ë•Œ ì˜ˆì¸¡ ë£¨í”„ ì¤‘ì§€
      if (animationFrameId.current) {
        cancelAnimationFrame(animationFrameId.current);
      }
    };
  }, [mediaStream]);

  // 3. ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë£¨í”„
  const startPredictionLoop = () => {
    const predict = async () => {
      const video = videoRef.current;
      const handLandmarker = handLandmarkerRef.current;
      const staticModel = staticModelRef.current;
      const dynamicModel = dynamicModelRef.current;

      if (
        !video ||
        !handLandmarker ||
        !staticModel ||
        !dynamicModel ||
        video.readyState < 2
      ) {
        animationFrameId.current = requestAnimationFrame(predict);
        return;
      }

      if (video.currentTime !== lastVideoTimeRef.current) {
        lastVideoTimeRef.current = video.currentTime;
        const handLandmarkerResult = handLandmarker.detectForVideo(
          video,
          Date.now()
        );

        if (
          handLandmarkerResult.landmarks &&
          handLandmarkerResult.landmarks.length > 0
        ) {
          const landmarks = handLandmarkerResult.landmarks[0];
          drawLandmarks(landmarks);

          // ë°ì´í„° ì „ì²˜ë¦¬ (ë‘ ëª¨ë¸ì´ ê³µí†µìœ¼ë¡œ ì‚¬ìš©)
          const wrist = landmarks[0];
          const keypoints = landmarks.flatMap((lm) => [
            lm.x - wrist.x,
            lm.y - wrist.y,
            lm.z - wrist.z,
          ]);

          // --- ì •ì  ëª¨ë¸ ì˜ˆì¸¡ (ë§¤ í”„ë ˆì„ ì‹¤í–‰) ---
          tf.tidy(() => {
            const inputTensor = tf.tensor2d([keypoints], [1, 63]);
            const prediction = staticModel.predict(inputTensor) as tf.Tensor;
            const predictedIndex = prediction.argMax(-1).dataSync()[0];
            const label = STATIC_LABELS[predictedIndex];
            setStaticGesture(`ì •ì : ${KOREAN_STATIC_LABELS[label] || label}`);
          });

          // --- ë™ì  ëª¨ë¸ ì˜ˆì¸¡ (ì‹œí€€ìŠ¤ê°€ ì°¼ì„ ë•Œ ì‹¤í–‰) ---
          sequenceRef.current.push(keypoints);
          sequenceRef.current = sequenceRef.current.slice(-SEQUENCE_LENGTH);

          if (sequenceRef.current.length === SEQUENCE_LENGTH) {
            tf.tidy(() => {
              const inputTensor = tf.tensor3d(
                [sequenceRef.current],
                [1, SEQUENCE_LENGTH, 63]
              );
              const prediction = dynamicModel.predict(inputTensor) as tf.Tensor;
              const predictionData = prediction.dataSync();
              const confidence = Math.max(...predictionData);

              let label = "none";
              if (confidence >= CONFIDENCE_THRESHOLD) {
                const predictedIndex = prediction.argMax(-1).dataSync()[0];
                label = DYNAMIC_LABELS[predictedIndex];
              }
              setDynamicGesture(
                `ë™ì : ${KOREAN_DYNAMIC_LABELS[label] || label}`
              );

              // TODO: ì—¬ê¸°ì— 'fire', 'nyan' ë“± ë™ì  ì œìŠ¤ì²˜ì— ë”°ë¥¸ ì‹œê° íš¨ê³¼ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            });
          }
        } else {
          clearCanvas();
          sequenceRef.current = []; // ì†ì´ ì•ˆë³´ì´ë©´ ì‹œí€€ìŠ¤ ì´ˆê¸°í™”
        }
      }
      animationFrameId.current = requestAnimationFrame(predict);
    };
    predict();
  };
  // ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° í•¨ìˆ˜
  const drawLandmarks = (landmarks: NormalizedLandmark[]) => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    landmarks.forEach((landmark) => {
      const x = landmark.x * canvas.width;
      const y = landmark.y * canvas.height;
      ctx.beginPath();
      ctx.arc(x, y, 5, 0, 2 * Math.PI);
      ctx.fillStyle = "aqua";
      ctx.fill();
    });
  };

  // ìº”ë²„ìŠ¤ í´ë¦¬ì–´ í•¨ìˆ˜
  const clearCanvas = () => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;
    ctx.clearRect(0, 0, canvas.width, canvas.height);
  };

  if (!mediaStream) {
    return <div>ë¡œì»¬ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...</div>;
  }

  return (
    <div style={{ position: "relative", width: "100%", height: "100%" }}>
      <video
        ref={videoRef}
        autoPlay
        playsInline
        muted
        style={{ width: "100%", height: "100%", transform: "scaleX(-1)" }}
      />
      <canvas
        ref={canvasRef}
        width="640"
        height="360"
        style={{
          position: "absolute",
          top: 0,
          left: 0,
          width: "100%",
          height: "100%",
          transform: "scaleX(-1)",
        }}
      />

      {/* ì •ì  ì œìŠ¤ì²˜ ê²°ê³¼ (ì¢Œì¸¡ í•˜ë‹¨) */}
      <div
        style={{
          position: "absolute",
          bottom: "10px",
          left: "10px",
          backgroundColor: "rgba(0,0,0,0.6)",
          color: "white",
          padding: "5px 10px",
          borderRadius: "5px",
          fontSize: "16px",
        }}
      >
        {staticGesture}
      </div>
      {/* ë™ì  ì œìŠ¤ì²˜ ê²°ê³¼ (ìš°ì¸¡ í•˜ë‹¨) */}
      <div
        style={{
          position: "absolute",
          bottom: "10px",
          right: "10px",
          backgroundColor: "rgba(0,0,0,0.6)",
          color: "white",
          padding: "5px 10px",
          borderRadius: "5px",
          fontSize: "16px",
        }}
      >
        {dynamicGesture}
      </div>
    </div>
  );
};
