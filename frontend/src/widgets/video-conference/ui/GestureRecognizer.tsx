// src/widgets/video-conference/ui/GestureRecognizer.tsx
"use client";

import { useEffect, useRef, useState } from "react";
import * as tf from "@tensorflow/tfjs";
import { HandLandmarker, FilesetResolver } from "@mediapipe/tasks-vision";
import type {
  HandLandmarkerResult,
  NormalizedLandmark,
} from "@mediapipe/tasks-vision";

// ì»´í¬ë„ŒíŠ¸ Props íƒ€ì… ì •ì˜
interface GestureRecognizerProps {
  mediaStream: MediaStream | null;
}

const GESTURE_LABELS = [
  "bad",
  "fist",
  "good",
  "gun",
  "heart",
  "none",
  "ok",
  "open_palm",
  "promise",
  "rock",
  "victory",
];

const KOREAN_LABELS: { [key: string]: string } = {
  bad: "ğŸ‘",
  fist: "ì£¼ë¨¹",
  good: "ğŸ‘",
  gun: "ì´ ëª¨ì–‘",
  heart: "ì†ê°€ë½ í•˜íŠ¸",
  none: "ì—†ìŒ",
  ok: "OK",
  open_palm: "ì†ë°”ë‹¥",
  promise: "ì•½ì†",
  rock: "ë½ì•¤ë¡¤",
  victory: "ë¸Œì´",
};

export const GestureRecognizer: React.FC<GestureRecognizerProps> = ({
  mediaStream,
}) => {
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const [gesture, setGesture] = useState<string>("ì¤€ë¹„ ì¤‘...");

  // MediaPipeì™€ TFJS ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ëŠ” ë¦¬ë Œë”ë§ë˜ì–´ë„ ìœ ì§€ë˜ë„ë¡ refë¡œ ê´€ë¦¬
  const handLandmarkerRef = useRef<HandLandmarker | null>(null);
  const modelRef = useRef<tf.LayersModel | null>(null);
  const lastVideoTimeRef = useRef<number>(-1);
  const animationFrameId = useRef<number | null>(null);

  // 1. ì´ˆê¸°í™” (ëª¨ë¸ ë¡œë”© ë° MediaPipe ì„¤ì •)
  useEffect(() => {
    async function setupModels() {
      try {
        // MediaPipe HandLandmarker ì´ˆê¸°í™”
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
        );
        const handLandmarker = await HandLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task`,
            delegate: "GPU",
          },
          runningMode: "VIDEO",
          numHands: 1, // ë¡œì»¬ ìœ ì €ì˜ í•œ ì†ë§Œ ì¸ì‹
        });
        handLandmarkerRef.current = handLandmarker;

        // TensorFlow.js ëª¨ë¸ ë¡œë”© (public í´ë” ê¸°ì¤€ ê²½ë¡œ)
        const model = await tf.loadLayersModel("/model/model.json");
        modelRef.current = model;

        setGesture("ì¹´ë©”ë¼ë¥¼ í–¥í•´ ì†ì„ ë³´ì—¬ì£¼ì„¸ìš”");
      } catch (error) {
        console.error("AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
        setGesture("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨");
      }
    }
    setupModels();

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ìì› ì •ë¦¬
    return () => {
      if (animationFrameId.current) {
        cancelAnimationFrame(animationFrameId.current);
      }
      handLandmarkerRef.current?.close();
      modelRef.current?.dispose();
    };
  }, []);

  // 2. MediaStreamì´ ë³€ê²½ë  ë•Œ ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ì— ì—°ê²°
  useEffect(() => {
    if (mediaStream && videoRef.current) {
      videoRef.current.srcObject = mediaStream;
      videoRef.current.addEventListener("loadeddata", () => {
        // ë¹„ë””ì˜¤ ì¬ìƒì´ ì‹œì‘ë˜ë©´ ì˜ˆì¸¡ ë£¨í”„ë¥¼ ì‹œì‘
        startPredictionLoop();
      });
    }

    return () => {
      // ìŠ¤íŠ¸ë¦¼ì´ ë°”ë€Œê±°ë‚˜ ì»´í¬ë„ŒíŠ¸ê°€ ì‚¬ë¼ì§ˆ ë•Œ ì˜ˆì¸¡ ë£¨í”„ ì¤‘ì§€
      if (animationFrameId.current) {
        cancelAnimationFrame(animationFrameId.current);
      }
    };
  }, [mediaStream]);

  // 3. ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë£¨í”„
  const startPredictionLoop = () => {
    const video = videoRef.current;
    if (!video || !handLandmarkerRef.current || !modelRef.current) return;

    const predict = async () => {
      // â–¼â–¼â–¼â–¼â–¼ ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ â–¼â–¼â–¼â–¼â–¼

      // refì˜ í˜„ì¬ ê°’ì„ ì§€ì—­ ìƒìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.
      const handLandmarker = handLandmarkerRef.current;
      const model = modelRef.current;

      // predict ë£¨í”„ê°€ ë§¤ë²ˆ ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
      // ì´ë ‡ê²Œ í•˜ë©´ TypeScriptë„ ì´ ë³€ìˆ˜ë“¤ì´ nullì´ ì•„ë‹˜ì„ ì¸ì§€í•©ë‹ˆë‹¤.
      if (!video || !handLandmarker || !model) {
        animationFrameId.current = requestAnimationFrame(predict);
        return;
      }

      if (
        video.readyState >= 2 &&
        video.currentTime !== lastVideoTimeRef.current
      ) {
        lastVideoTimeRef.current = video.currentTime;

        // ì´ì œ ì•ˆì „í•˜ê²Œ ì§€ì—­ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        const handLandmarkerResult = handLandmarker.detectForVideo(
          video,
          Date.now()
        );

        if (
          handLandmarkerResult.landmarks &&
          handLandmarkerResult.landmarks.length > 0
        ) {
          const landmarks = handLandmarkerResult.landmarks[0];
          drawLandmarks(landmarks);

          // 1. ì†ëª©(landmark 0)ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
          const wrist = landmarks[0];

          // 2. ëª¨ë“  ëœë“œë§ˆí¬ ì¢Œí‘œë¥¼ ì†ëª© ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
          //    Pythonì˜ extract_keypoints ë¡œì§ê³¼ ë™ì¼í•©ë‹ˆë‹¤.
          const inputData = landmarks.flatMap((lm) => [
            lm.x - wrist.x,
            lm.y - wrist.y,
            lm.z - wrist.z,
          ]);

          // 3. ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” shape [1, 63]ìœ¼ë¡œ í…ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
          const inputTensor = tf.tensor2d([inputData], [1, 63]);

          // modelë„ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
          const prediction = model.predict(inputTensor) as tf.Tensor;
          const predictionData = await prediction.data();

          const predictedIndex = tf.argMax(predictionData).dataSync()[0];
          // 1. ì˜ì–´ ë ˆì´ë¸”ì„ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
          const englishLabel = GESTURE_LABELS[predictedIndex] || "ì•Œ ìˆ˜ ì—†ìŒ";

          // 2. ì˜ì–´ ë ˆì´ë¸”ì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ë ˆì´ë¸”ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
          const koreanLabel = KOREAN_LABELS[englishLabel] || englishLabel;

          // 3. UI ìƒíƒœë¥¼ í•œêµ­ì–´ ë ˆì´ë¸”ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
          setGesture(koreanLabel);

          inputTensor.dispose();
          prediction.dispose();
        } else {
          clearCanvas();
        }
      }
      animationFrameId.current = requestAnimationFrame(predict);
    };
    predict();
  };

  // ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° í•¨ìˆ˜
  const drawLandmarks = (landmarks: NormalizedLandmark[]) => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    landmarks.forEach((landmark) => {
      const x = landmark.x * canvas.width;
      const y = landmark.y * canvas.height;
      ctx.beginPath();
      ctx.arc(x, y, 5, 0, 2 * Math.PI);
      ctx.fillStyle = "aqua";
      ctx.fill();
    });
  };

  // ìº”ë²„ìŠ¤ í´ë¦¬ì–´ í•¨ìˆ˜
  const clearCanvas = () => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;
    ctx.clearRect(0, 0, canvas.width, canvas.height);
  };

  if (!mediaStream) {
    return <div>ë¡œì»¬ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...</div>;
  }

  return (
    <div style={{ position: "relative", width: "100%", height: "100%" }}>
      {/* 
        ì‹¤ì œ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ (ì¢Œìš° ë°˜ì „ìœ¼ë¡œ ê±°ìš¸ ëª¨ë“œ)
        ì´ ë¹„ë””ì˜¤ëŠ” MediaPipeì— ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ì†ŒìŠ¤ ì—­í• ì„ í•©ë‹ˆë‹¤.
      */}
      <video
        ref={videoRef}
        autoPlay
        playsInline
        muted
        style={{ width: "100%", height: "100%", transform: "scaleX(-1)" }}
      />
      {/* 
        ëœë“œë§ˆí¬ë¥¼ ê·¸ë¦´ ìº”ë²„ìŠ¤ (ë¹„ë””ì˜¤ ìœ„ì— ì˜¤ë²„ë ˆì´)
        ë¹„ë””ì˜¤ì™€ ë™ì¼í•˜ê²Œ ì¢Œìš° ë°˜ì „ ì‹œì¼œ ì¢Œí‘œë¥¼ ë§ì¶¥ë‹ˆë‹¤.
      */}
      <canvas
        ref={canvasRef}
        width="640"
        height="360" // ë¹„ë””ì˜¤ í•´ìƒë„ì— ë§ì¶° ì¡°ì •
        style={{
          position: "absolute",
          top: 0,
          left: 0,
          width: "100%",
          height: "100%",
          transform: "scaleX(-1)",
        }}
      />
      {/* ì¸ì‹ëœ ì œìŠ¤ì²˜ í‘œì‹œ */}
      <div
        style={{
          position: "absolute",
          bottom: "10px",
          left: "10px",
          backgroundColor: "rgba(0,0,0,0.6)",
          color: "white",
          padding: "5px 10px",
          borderRadius: "5px",
        }}
      >
        {gesture}
      </div>
    </div>
  );
};
